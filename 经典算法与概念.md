# MDP框架

## MDP——马尔可夫决策过程

1. 马尔科夫性

   所谓马尔科夫性是指系统的下一个状态st+1仅与当前状态st有关，而与以前的状态无关。

2. 马尔科夫过程

   马尔科夫过程是一个二元组(S,P)，且满足：S是有限状态集合， P是状态转移概率

3. 马尔科夫决策过程

   1. 马尔科夫决策过程由元组(S,A,P,R,γ)描述，其中：S为有限的状态集, A 为有限的动作集, P 为状态转移概率, R为回报函数, γ 为折扣因子，用来计算累积回报。注意，跟马尔科夫过程不同的是，马尔科夫决策过程的状态转移概率是包含动作的即：

   $$
   P_{ss'}^a=P[S_{t+1}=s'|S_t=s,A_t=a]
   $$

   2. 策略π的定义, 指给定状态s 时，动作集上的一个分布
      $$
      π(a|s)=p[A_t=a|S_t=s] (1.1)
      $$

   3. 累计回报：
      $$
      G_t=R_{t+1}+γR_t+2+⋯=∑_{k=0}^∞γ^kR_{t+k+1} (1.2)
      $$

   4. 状态值函数，当智能体采用策略π时，累积回报服从一个分布，累积回报在状态s处的期望值定义为状态-值函数：
      $$
      υ_π(s)=E_π[∑_{k=0}^∞γ^kR_{t+k+1}|S_t=s] (1.3)
      $$

   5. 状态-行为值函数：
      $$
      q_π(s,a)=E_π[∑_{k=0}^∞γ^kR_{t+k+1}|S_t=s,A_t=a] (1.4)
      $$

   6. **状态值函数与状态-行为值函数的贝尔曼方程**
      $$
      υ(s)&= & E[Gt|St=s]\\
          &= & E[R_{t+1}+γR_{t+2}+⋯|St=s]\\
          &= & E[R_{t+1}+γ(R_{t+2}+γR_{t+3}+⋯)|St=s]\\
          &= & E[R_{t+1}+γG_{t+1}|St=s\\
          &= & E[R_{t+1}+γυ(S_{t+1})|St=s] (1.5)
      $$

      $$
      q_π(s,a)=E_π[Rt+1+γq(S_{t+1},A_{t+1})|S_t=s,A_t=a] (1.6)
      $$
      
      $$
      υ_π(s)=∑_{a∈A}π(a|s)(R_s^a+γ∑_{s'∈S}P_{ss'}^aυ_{π(s')})(1.9)
      $$
      
      $$
      q_π(s,a)=R_s^a+γ∑_{s′∈S}P_{ss'}^a∑_{a'∈A}π(a'|s')q_π(s',a')(1.11)
      $$
      
   7. 最优状态值函数$υ^*(s)$,为在所有策略中值最大的值函数即：$$υ^∗(s)=max_πυ_π(s)$$，最优状态-行为值函数$q^∗(s,a)$为在所有策略中最大的状态-行为值函数，即：$q^∗(s,a)=max_πq_π(s,a)$
   
   8. 我们由(1.9)式和(1.11)式分别得到最优状态值函数和最优状态-行动值函数的贝尔曼最优方程：
   
      ​	$υ^∗(s)=max_{a}R_s^a+γ∑_{s'∈S}P_{ss'}^aυ^∗(s')(1.12)$
   
      ​	$q^∗(s,a)=R_s^a+γ∑_{s'∈S}P_{ss'}^{a}max_{a'}q^∗(s',a')(1.13)$
      $$
      π^∗(a|s)= 
      \begin{cases}
      1 \ if \ a=arg \ maxq^∗(s,a) , \ a∈A\\
      0 \ otherwis 
      \end{cases}
      $$
      

## POMDP

1.模型定义

![image-20221209160612133](C:\Users\你安吧\AppData\Roaming\Typora\typora-user-images\image-20221209160612133.png)

2. 置信空间

   ![image-20221209161006231](C:\Users\你安吧\AppData\Roaming\Typora\typora-user-images\image-20221209161006231.png)

3. 优化目标

   ![image-20221209161205717](C:\Users\你安吧\AppData\Roaming\Typora\typora-user-images\image-20221209161205717.png)

## ρ-POMDP

在POMDP的基础上，根据A-SLAM的特点修改

# 熵的概念

定义：In [information theory](https://en.wikipedia.org/wiki/Information_theory), the **entropy** of a [random variable](https://en.wikipedia.org/wiki/Random_variable) is the average level of "information", "surprise", or "uncertainty" inherent to the variable's possible outcomes.

$H(X)=−∑_{i=1}^np(xi)log(p(xi))$

## (Mutual Information)MI

- 
  I(X;Y)=H(X)−H(X|Y)=H(Y)−H(Y|X)=H(X)+H(Y)−H(X,Y)

获取X信息后，对获取Y信息有多大的帮助

## Kullback-Leibler divergence）KLD

又叫相对熵

**量化两种概率分布P和Q之间差异**

![img](https://upload-images.jianshu.io/upload_images/75110-e94b5412d85e5698.png?imageMogr2/auto-orient/strip|imageView2/2/w/752/format/webp)

讲解文章：

[如何理解K-L散度]: https://www.jianshu.com/p/43318a3dc715

# WFD && FFD

- [Matan Keidar](https://www.semanticscholar.org/author/Matan-Keidar/2355983), [G. Kaminka](https://www.semanticscholar.org/author/G.-Kaminka/1725049)
- Published in AAMAS 4 June 2012

<Robot Exploration with Fast Frontier Detection: Theory and Experiments>

![image-20221209172410572](C:\Users\你安吧\AppData\Roaming\Typora\typora-user-images\image-20221209172410572.png)

![image-20221209172350239](C:\Users\你安吧\AppData\Roaming\Typora\typora-user-images\image-20221209172350239.png)

# 强化学习RL

1. 监督学习

   1. 选择一个适合目标任务的数学模型
   2. 先把一部分已知的“问题和答案”（训练集）给机器去学习
   3. 机器总结出了自己的“方法论”
   4. 人类把”新的问题”（测试集）给机器，让他去解答

2. 无监督学习

   **无监督学习是一种机器学习的训练方式，它本质上是一个统计手段，在没有标签的数据里可以发现潜在的一些结构的一种训练方式**

   1. 与监督学习的区别
      1. 监督学习是一种目的明确的训练方式，你知道得到的是什么；而**无监督学习则是没有明确目的的训练方式，你无法提前知道结果是什么**。
      2. 监督学习需要给数据打标签；而**无监督学习不需要给数据打标签**。
      3. 监督学习由于目标明确，所以可以衡量效果；而**无监督学习几乎无法量化效果如何。**

   2. 常用算法
      1. 聚类：简单说就是一种自动分类的方法，在监督学习中，你很清楚每一个分类是什么，但是聚类则不是，你并不清楚聚类后的几个分类每个代表什么意思。**如：K均值聚类**
      2. 降维：降维看上去很像压缩。这是为了在尽可能保存相关的结构的同时降低数据的复杂度。如： **PCA**

3. 强化学习
   1. **免模型学习（Model-Free） vs 有模型学习（Model-Based）**
   2. 片段性任务 / 连续性任务
   3. 强化学习三种方法
      1. 基于价值
      2. 基于策略
      3. 基于模型

4. 深度强化学习

   **深度强化学习**，**就是在强化学习里**，**加入深度神经网络**

1. 深度学习
2. FAST-LIO建图算法
3. 开源代码

# 支持向量机（SVM）

*定义：*[*支持向量机（英语：support vector machine，常简称为SVM，又名支持向量网络）是在分类与回归分析中分析数据的监督式学习模型与相关的学习算法。给定一组训练实例，每个训练实例被标记为属于两个类别中的一个或另一个，SVM训练算法创建一个将新的实例分配给两个类别之一的模型，使其成为非概率二元线性分类器。SVM模型是将实例表示为空间中的点，这样映射就使得单独类别的实例被尽可能宽的明显的间隔分开。然后，将新的实例映射到同一空间，并基于它们落在间隔的哪一侧来预测所属类别*。]

## 0. 感知机

1. **定义**：*感知机是一种二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，+1代表正类，-1代表负类。感知机属于判别模型，它的目标是要将输入实例通过分离超平面将正负二类分离。*

2. **感知机模型**：$h(x) = sign(∑^d_{i=1}w_ix_i-threshold)$

3. **感知机的几何解释**：线性方程$wx+b=0$对应于特征空间中的一个超平面S，其中w是超平面的法向量，b是超平面的截距。这个超平面将特征空间分为了两个部分。位于两部分的点(即特征向量)分别被分为正、负两类。S也被称之为分离超平面(separating hyperplane)

4. **感知机学习**是有监督学习过程，通过有label的训练数据集求的模型的参数w，b。

   **感知机预测过程**：对于新输入实例的特征向量，输入到**$h(x)=sign(wx+b)$**，输出对应类别。

5. **数据的线性可分性**:给定一个数据集，如果存在某个超平面$S(wx+b=0)$，能够将数据集的正实例点和负实例点完全正确的划分到超平面的两侧，即对$y=+1$的实例点，有$wx+b>0$，对$y=-1$的实例点，有$wx+b<0=0$，则称数据集线性可分，否则线性不可分。

6. **感知机学习策略**:

   1. 范数: L-P范数

      - L0范数 通常表示向量中非零元素的个数
      - L1范数 又叫**曼哈顿距离、最小绝对误差**等。使用 **L1范数可以度量两个向量间的差异，如绝对误差和（Sum of Absolute Difference）**
      - L2范数 最多的度量**距离欧氏距离就是一种L2范数**

   2. 损失函数，误分类点到超平面S的总距离

      $ L(w,b)=−∑_{xi∈M}yi∗(w∗xi+b)$

7. 感知机学习算法

## 1. 线性可分SVM——硬间隔

1. 超平面与间隔

   X·W+b=0

   ![img](https://pic1.zhimg.com/80/v2-bfc8c57f522ce521994035859fcb388c_1440w.webp)

2. 间隔最大化

   $min(1/2)||W||^2$

3. 支持向量

   在线性可分的情况下，训练数据集的样本点中与分离超平面距离最近的数据点称为**支持向量(support vector)**

​		![img](https://pic2.zhimg.com/80/v2-f9e1e7fd08460a5fab044c71ed8b0bb1_1440w.webp)

4. 对偶问题

   应用拉格朗日乘子法构造拉格朗日函数(Lagrange function)再通过求解其对偶问题(dual problem)得到原始问题的最优解。

   [对偶问题]: https://zhuanlan.zhihu.com/p/49331510

   

## 2. 线性SVM——软间隔

*允许少量样本犯错*

1. 惩罚项: hinge损失$l_{hinge}(z)=max(0,1−z)$，加入惩罚项，新的优化目标为

   $min_{W,b}\ \ 1/2||W||^2+C∑_{i=1}^nmax(0,1−y_i(X_i^TW+b))$

   其中C>0称为惩罚参数，C越小时对误分类惩罚越小，越大时对误分类惩罚越大，当C取正无穷时就变成了硬间隔优化。实际应用时我们要合理选取C，C越小越容易欠拟合，C越大越容易过拟合。

## 3. 非线性SVM——核技巧

*核技巧的基本思路分为两步:使用一个变换**将原空间的数据映射到新空间**(例如更高维甚至无穷维的空间)；然后**在新空间里用线性方法**从训练数据中学习得到模型。*

![img](https://pic1.zhimg.com/80/v2-87d10648809c0b6c5e473bd4565c2c08_1440w.webp)

1. ###### 核函数

   $设X是输入空间(欧式空间Rn的子集或离散集合)，又设H是特征空间(希尔伯特空间)，\\如果存在一个X到H的映射ϕ(x):X→H使得对所有x,z∈X，函数K(x,z)满足条件K(x,z)=ϕ(x)⋅ϕ(z)\\则称K(x,z)为核函数，ϕ(x)为映射函数，式中ϕ(x)⋅ϕ(z)为ϕ(x)和ϕ(z)的內积$

2. 正定核

$设X⊂Rn,K(x,z)是定义在X×X上的对称函数，如果对任意的xi∈X,i=1,2,...,m，\\K(x,z)对应的Gram矩阵K=[K(xi,xj)]m×m是半正定矩阵，则K(x,z)是正定核$

# 相关向量机（RVM）

## 0.支持向量机局限性

1. SVM的输出是一个决策结果而并不是后验概率。
2. SVM最开始是用来处理二分类问题，当推广到多分类问题时，需要用到“1v1”，“1vo”等策略，计算复杂度增加。
3. 复杂度参数C(见带有松弛系数的支持向量机对偶公式)或者v-SVM中的参数v等需要诸如交叉验证等方法来确定。这需要涉及到多次的训练过程。
4. 预测的表示形式为核函数的线性组合，核函数是以训练数据点为中心，且必须正定。
