# MDP框架

## MDP——马尔可夫决策过程

1. 马尔科夫性

   所谓马尔科夫性是指系统的下一个状态st+1仅与当前状态st有关，而与以前的状态无关。

2. 马尔科夫过程

   马尔科夫过程是一个二元组(S,P)，且满足：S是有限状态集合， P是状态转移概率

3. 马尔科夫决策过程

   1. 马尔科夫决策过程由元组(S,A,P,R,γ)描述，其中：S为有限的状态集, A 为有限的动作集, P 为状态转移概率, R为回报函数, γ 为折扣因子，用来计算累积回报。注意，跟马尔科夫过程不同的是，马尔科夫决策过程的状态转移概率是包含动作的即：

   $$
   P_{ss'}^a=P[S_{t+1}=s'|S_t=s,A_t=a]
   $$

   2. 策略π的定义, 指给定状态s 时，动作集上的一个分布
      $$
      π(a|s)=p[A_t=a|S_t=s] (1.1)
      $$

   3. 累计回报：
      $$
      G_t=R_{t+1}+γR_t+2+⋯=∑_{k=0}^∞γ^kR_{t+k+1} (1.2)
      $$

   4. 状态值函数，当智能体采用策略π时，累积回报服从一个分布，累积回报在状态s处的期望值定义为状态-值函数：
      $$
      υ_π(s)=E_π[∑_{k=0}^∞γ^kR_{t+k+1}|S_t=s] (1.3)
      $$

   5. 状态-行为值函数：
      $$
      q_π(s,a)=E_π[∑_{k=0}^∞γ^kR_{t+k+1}|S_t=s,A_t=a] (1.4)
      $$

   6. **状态值函数与状态-行为值函数的贝尔曼方程**
      $$
      υ(s)&= & E[Gt|St=s]\\
          &= & E[R_{t+1}+γR_{t+2}+⋯|St=s]\\
          &= & E[R_{t+1}+γ(R_{t+2}+γR_{t+3}+⋯)|St=s]\\
          &= & E[R_{t+1}+γG_{t+1}|St=s\\
          &= & E[R_{t+1}+γυ(S_{t+1})|St=s] (1.5)
      $$

      $$
      q_π(s,a)=E_π[Rt+1+γq(S_{t+1},A_{t+1})|S_t=s,A_t=a] (1.6)
      $$
      
      $$
      υ_π(s)=∑_{a∈A}π(a|s)(R_s^a+γ∑_{s'∈S}P_{ss'}^aυ_{π(s')})(1.9)
      $$
      
      $$
      q_π(s,a)=R_s^a+γ∑_{s′∈S}P_{ss'}^a∑_{a'∈A}π(a'|s')q_π(s',a')(1.11)
      $$
      
   7. 最优状态值函数$υ^*(s)$,为在所有策略中值最大的值函数即：$$υ^∗(s)=max_πυ_π(s)$$，最优状态-行为值函数$q^∗(s,a)$为在所有策略中最大的状态-行为值函数，即：$q^∗(s,a)=max_πq_π(s,a)$
   
   8. 我们由(1.9)式和(1.11)式分别得到最优状态值函数和最优状态-行动值函数的贝尔曼最优方程：
   
      ​	$υ^∗(s)=max_{a}R_s^a+γ∑_{s'∈S}P_{ss'}^aυ^∗(s')(1.12)$
   
      ​	$q^∗(s,a)=R_s^a+γ∑_{s'∈S}P_{ss'}^{a}max_{a'}q^∗(s',a')(1.13)$
      $$
      π^∗(a|s)= 
      \begin{cases}
      1 \ if \ a=arg \ maxq^∗(s,a) , \ a∈A\\
      0 \ otherwis 
      \end{cases}
      $$
      

## POMDP

1.模型定义

![image-20221209160612133](C:\Users\你安吧\AppData\Roaming\Typora\typora-user-images\image-20221209160612133.png)

2. 置信空间

   ![image-20221209161006231](C:\Users\你安吧\AppData\Roaming\Typora\typora-user-images\image-20221209161006231.png)

3. 优化目标

   ![image-20221209161205717](C:\Users\你安吧\AppData\Roaming\Typora\typora-user-images\image-20221209161205717.png)

## ρ-POMDP

在POMDP的基础上，根据A-SLAM的特点修改

# 熵的概念

定义：In [information theory](https://en.wikipedia.org/wiki/Information_theory), the **entropy** of a [random variable](https://en.wikipedia.org/wiki/Random_variable) is the average level of "information", "surprise", or "uncertainty" inherent to the variable's possible outcomes.

$H(X)=−∑_{i=1}^np(xi)log(p(xi))$

## (Mutual Information)MI

- 
  I(X;Y)=H(X)−H(X|Y)=H(Y)−H(Y|X)=H(X)+H(Y)−H(X,Y)

获取X信息后，对获取Y信息有多大的帮助

## Kullback-Leibler divergence）KLD

又叫相对熵

**量化两种概率分布P和Q之间差异**

![img](https://upload-images.jianshu.io/upload_images/75110-e94b5412d85e5698.png?imageMogr2/auto-orient/strip|imageView2/2/w/752/format/webp)

讲解文章：

[如何理解K-L散度]: https://www.jianshu.com/p/43318a3dc715

# WFD && FFD

- [Matan Keidar](https://www.semanticscholar.org/author/Matan-Keidar/2355983), [G. Kaminka](https://www.semanticscholar.org/author/G.-Kaminka/1725049)
- Published in AAMAS 4 June 2012

<Robot Exploration with Fast Frontier Detection: Theory and Experiments>

![image-20221209172410572](C:\Users\你安吧\AppData\Roaming\Typora\typora-user-images\image-20221209172410572.png)

![image-20221209172350239](C:\Users\你安吧\AppData\Roaming\Typora\typora-user-images\image-20221209172350239.png)

# 强化学习RL

1. 监督学习

   1. 选择一个适合目标任务的数学模型
   2. 先把一部分已知的“问题和答案”（训练集）给机器去学习
   3. 机器总结出了自己的“方法论”
   4. 人类把”新的问题”（测试集）给机器，让他去解答

2. 无监督学习

   **无监督学习是一种机器学习的训练方式，它本质上是一个统计手段，在没有标签的数据里可以发现潜在的一些结构的一种训练方式**

   1. 与监督学习的区别
      1. 监督学习是一种目的明确的训练方式，你知道得到的是什么；而**无监督学习则是没有明确目的的训练方式，你无法提前知道结果是什么**。
      2. 监督学习需要给数据打标签；而**无监督学习不需要给数据打标签**。
      3. 监督学习由于目标明确，所以可以衡量效果；而**无监督学习几乎无法量化效果如何。**

   2. 常用算法
      1. 聚类：简单说就是一种自动分类的方法，在监督学习中，你很清楚每一个分类是什么，但是聚类则不是，你并不清楚聚类后的几个分类每个代表什么意思。**如：K均值聚类**
      2. 降维：降维看上去很像压缩。这是为了在尽可能保存相关的结构的同时降低数据的复杂度。如： **PCA**

3. 强化学习
   1. **免模型学习（Model-Free） vs 有模型学习（Model-Based）**
   2. 片段性任务 / 连续性任务
   3. 强化学习三种方法
      1. 基于价值
      2. 基于策略
      3. 基于模型

4. 深度强化学习

   **深度强化学习**，**就是在强化学习里**，**加入深度神经网络**

1. 深度学习
2. FAST-LIO建图算法
3. 开源代码
